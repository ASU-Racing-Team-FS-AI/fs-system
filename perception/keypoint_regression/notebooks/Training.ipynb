{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from torch.nn import Module, MSELoss, L1Loss\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from dataset import  KeypointDataset\n",
    "\n",
    "from model import KeypointNet\n",
    "\n",
    "from trainer import TrainingParameters, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgsPath = \"../data/imgs\"\n",
    "jsonPath = \"../data/json\"\n",
    "\n",
    "dataset = KeypointDataset(jsonPath, imgsPath,0,0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing dataloaders\n",
    "TRAIN_SIZE = int(0.7*len(dataset))\n",
    "VALID_SIZE = int(0.2*len(dataset))\n",
    "TEST_SIZE = len(dataset) - TRAIN_SIZE - VALID_SIZE\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# random split the data\n",
    "trainDataset, valDataset, testDataset = random_split(dataset, [TRAIN_SIZE, VALID_SIZE, TEST_SIZE])\n",
    "\n",
    "# creating dataloaders\n",
    "trainLoader = DataLoader(trainDataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validLoader = DataLoader(valDataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "testLoader = DataLoader(testDataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# sanity check\n",
    "print('='*30)\n",
    "print(\"Train dataset\", len(trainDataset))\n",
    "print(\"Valid dataset\", len(valDataset))\n",
    "print(\"Test dataset\", len(testDataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def showImageTensor(img):\n",
    "    img = img.detach().squeeze(0).permute((1,2,0)).cpu().numpy()\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "import numpy as np\n",
    "\n",
    "imgs, labels = next(iter(testLoader))\n",
    "i = 0\n",
    "bs, _, height, width = imgs.shape\n",
    "kps = labels[i].cpu().detach().numpy()\n",
    "print(kps)\n",
    "kps = kps.reshape(-1,2)\n",
    "kps[:,0] = kps[:,0]*width\n",
    "kps[:,1] = kps[:,1]*height\n",
    "kps = kps.astype(int)\n",
    "\n",
    "img = imgs[i].cpu().detach().permute(1,2,0).squeeze(2).numpy().astype(np.float32).copy()\n",
    "for point1 in kps:\n",
    "    plt.plot(*point1, marker='*', color='red', alpha=0.5)\n",
    "print(kps)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossRatioLoss(Module):\n",
    "    EPSILON = 1e-7\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, y1, y2):\n",
    "        y1 = y1.view(-1,8,2)\n",
    "        y2 = y2.view(-1,8,2)\n",
    "        \n",
    "        # extract slant heights\n",
    "        leftSlantPointsPreds = y1[:,:4,:]\n",
    "        leftSlantPointsGT = y2[:,:4,:]\n",
    "        rightSlantPointsPreds = y1[:,4:,:]\n",
    "        rightSlantPointsGT = y2[:,4:,:]\n",
    "        #print(leftSlantPointsPreds.shape, leftSlantPointsGT.shape, rightSlantPointsPreds.shape,\\\n",
    "        #rightSlantPointsGT.shape)\n",
    "        \n",
    "        # cone top (p1)\n",
    "        topLeftPointPreds = leftSlantPointsPreds[:,-1,:]\n",
    "        topRightPointPreds = rightSlantPointsPreds[:,0,:]\n",
    "        topLeftPointGT = leftSlantPointsGT[:,-1,:]\n",
    "        topRightPointGT = rightSlantPointsGT[:,0,:]\n",
    "        \n",
    "        p1Preds = ((topLeftPointPreds+topRightPointPreds)/2).unsqueeze(1).expand(-1,2,2)\n",
    "        p1GT = ((topLeftPointGT+topRightPointGT)/2).unsqueeze(1).expand(-1,2,2)\n",
    "        \n",
    "        # extract p2 and p5\n",
    "        p2Preds = leftSlantPointsPreds[:,1,:].unsqueeze(1).expand(-1,2,2)\n",
    "        p5Preds = rightSlantPointsPreds[:,1,:].unsqueeze(1).expand(-1,2,2)\n",
    "        p2GT = leftSlantPointsGT[:,1,:].unsqueeze(1).expand(-1,2,2)\n",
    "        p5GT = rightSlantPointsGT[:,1,:].unsqueeze(1).expand(-1,2,2)\n",
    "        \n",
    "        #print(p1Preds.shape, p1GT.shape)\n",
    "        \n",
    "        # extract deltas\n",
    "        deltaP1LeftPreds = torch.sqrt(torch.sum((p1Preds - leftSlantPointsPreds[:,3:,:])**2, axis=2))\n",
    "        deltaP1RightPreds = torch.sqrt(torch.sum((p1Preds - rightSlantPointsPreds[:,3:,:])**2, axis=2))\n",
    "        #deltaP1LeftGT = torch.sqrt(torch.sum((p1GT - leftSlantPointsGT[:,3:,:])**2, axis=2))\n",
    "        #deltaP1RightGT = torch.sqrt(torch.sum((p1GT - rightSlantPointsGT[:,3:,:])**2, axis=2))\n",
    "\n",
    "        deltaP2LeftPreds = torch.sqrt(torch.sum((p2Preds - leftSlantPointsPreds[:,3:,:])**2, axis=2))\n",
    "        deltaP5RightPreds = torch.sqrt(torch.sum((p5Preds - rightSlantPointsPreds[:,3:,:])**2, axis=2))\n",
    "        #deltaP2LeftGT = torch.sqrt(torch.sum((p2GT - leftSlantPointsGT[:,3:,:])**2, axis=2))\n",
    "        #deltaP5RightGT = torch.sqrt(torch.sum((p5GT - rightSlantPointsGT[:,3:,:])**2, axis=2))\n",
    "\n",
    "        crPredsLeft = self._calculateCrossRatio(deltaP1LeftPreds, deltaP2LeftPreds)\n",
    "        crPredsRight = self._calculateCrossRatio(deltaP1RightPreds, deltaP5RightPreds)\n",
    "        #crPredsLeftGT = self._calculateCrossRatio(deltaP1LeftGT, deltaP2LeftGT)\n",
    "        #crPredsRightGT = self._calculateCrossRatio(deltaP1RightGT, deltaP5RightGT)\n",
    "        \n",
    "        batchSize = y1.shape[0]\n",
    "        mseLeft = torch.sum((crPredsLeft-1.3940842428872968)**2)\n",
    "        mseRight = torch.sum((crPredsRight-1.3940842428872968)**2)\n",
    "        \n",
    "        return (mseLeft+mseRight)/batchSize\n",
    "    \n",
    "    def _calculateCrossRatio(self, delta1, delta2):\n",
    "        return (delta1[:,0]*delta2[:,1])/((delta1[:,1]*delta2[:,0])+CrossRatioLoss.EPSILON)\n",
    "    \n",
    "    \n",
    "class CombinedLoss(Module):\n",
    "    def __init__(self, gamma:float=0.5):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.mse = MSELoss()\n",
    "        self.cr = CrossRatioLoss()\n",
    "        self.l1loss = L1Loss()\n",
    "    \n",
    "    def forward(self, y1, y2):\n",
    "        mse = self.mse(y1,y2)\n",
    "        #cr = self.cr(y1,y2)\n",
    "        #return mse+self.gamma*cr\n",
    "        l1 = self.l1loss(y1,y2)\n",
    "        return mse+0.5*l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_,y1 = next(iter(trainLoader))\n",
    "_,y2 = next(iter(trainLoader))\n",
    "# print(y1.shape,y2.shape)\n",
    "criterion = CombinedLoss()\n",
    "criterion(y1,y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeypointNet(inputChannels=1, outKeypoints=8)\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=25, verbose=True)\n",
    "trainingTracker = []\n",
    "validTracker = []\n",
    "trainingParameters = TrainingParameters(\n",
    "    model=model,\n",
    "    criterion=CombinedLoss(0.1),\n",
    "    optimizer=optimizer,\n",
    "    trainloader=trainLoader,\n",
    "    validloader=validLoader,\n",
    "    epochs=2000,\n",
    "    savedModelName= \"checkpoints/mseloss2.pth\",\n",
    "    scheduler=scheduler,\n",
    "    trainingTracker=trainingTracker,\n",
    "    validTracker=validTracker\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=25, verbose=True)\n",
    "trainingParameters = TrainingParameters(\n",
    "    model=model,\n",
    "    criterion=CombinedLoss(0.05),\n",
    "    optimizer=optimizer,\n",
    "    trainloader=trainLoader,\n",
    "    validloader=validLoader,\n",
    "    epochs=2000,\n",
    "    savedModelName= \"checkpoints/mseloss3.pth\",\n",
    "    scheduler=scheduler,\n",
    "    trainingTracker=trainingTracker,\n",
    "    validTracker=validTracker\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train(trainingParameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trainingTracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation plots\n",
    "import numpy as np\n",
    "START = 2\n",
    "LIMIT = 457\n",
    "epochs = np.arange(LIMIT-START)\n",
    "plt.plot(epochs, trainingTracker[START:LIMIT], label=\"Training\")\n",
    "plt.plot(epochs, validTracker[START:LIMIT], label=\"Validation\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# test loss\n",
    "model = KeypointNet(inputChannels=1, outKeypoints=8)\n",
    "model.load_state_dict(torch.load(\"checkpoints/mseloss2.pth\"))\n",
    "model = model.cuda()\n",
    "testloss = 0\n",
    "criterion = CombinedLoss()\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for imgs, labels in testLoader:\n",
    "        batchSize, *_ =imgs.shape\n",
    "        out = model(imgs.cuda())\n",
    "        loss = criterion(out, labels.cuda())\n",
    "        testloss+=loss.item()*batchSize\n",
    "        \n",
    "print(\"Loss:\", testloss/len(testLoader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "imgs, labels = next(iter(testLoader))\n",
    "\n",
    "for j, (imgs, labels) in enumerate(testLoader):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        model = model.cuda()\n",
    "        outs = model(imgs.cuda())\n",
    "\n",
    "    bs, _, height, width = imgs.shape\n",
    "    fig, ax = plt.subplots(nrows=8, ncols=8, figsize=(20,20))\n",
    "\n",
    "    for i, axi in enumerate(ax.flat):\n",
    "        # get keypoints\n",
    "        try:\n",
    "            kps = outs[i].cpu().detach().numpy()\n",
    "            kps = kps.reshape(-1,2)\n",
    "            kps[:,0] = kps[:,0]*width\n",
    "            kps[:,1] = kps[:,1]*height\n",
    "            kps = kps.astype(int)\n",
    "            # labels\n",
    "            label = labels[i].cpu().detach().numpy()\n",
    "            label = label.reshape(-1,2)\n",
    "            label[:,0] = label[:,0]*width\n",
    "            label[:,1] = label[:,1]*height\n",
    "            label = label.astype(int)\n",
    "            # get img\n",
    "            img = imgs[i].cpu().detach().permute(1,2,0).squeeze(2).numpy().astype(np.float32).copy()\n",
    "\n",
    "            for point1, point2 in zip(kps, label):\n",
    "                axi.plot(*point1, marker='.', color='red', alpha=0.5)\n",
    "                axi.plot(*point2, marker='*', color='blue', alpha=0.5)\n",
    "            axi.imshow(img)\n",
    "        except: \n",
    "            continue\n",
    "\n",
    "    fig.show()\n",
    "    fig.savefig(f\"res_{j}\"\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "#     singleInput = imgs[i].unsqueeze(0).cuda()\n",
    "#     kpGT = labels[i].unsqueeze(0).cuda()\n",
    "#     singleInput.requires_grad = True\n",
    "#     kpVector = model(singleInput)\n",
    "#     loss = criterion(kpGT, kpVector)\n",
    "#     loss.backward()\n",
    "#     mask = singleInput.grad.abs()\n",
    "#     mask = mask/mask.max()\n",
    "#     viz = singleInput+0.5*mask\n",
    "#     viz = viz/viz.max()\n",
    "#     plt.imshow(viz.detach().squeeze(0).squeeze(0).cpu().numpy(), cmap='gray')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Extracts bounding boxes from the image and preprocesses them for the model.\n",
    "\"\"\"\n",
    "from typing import List, Dict, Union\n",
    "\n",
    "import torch\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class ImagePreprocessing:\n",
    "    \"\"\"This class is used to preprocess the image and the bounding boxes\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Constructor for the class\n",
    "        parameters:\n",
    "        -----------\n",
    "        bboxesImages: list\n",
    "             a list of cropped bounding box images.\n",
    "        topLeft :torch.Tensor\n",
    "             a tensor of top-left corner coordinates of the bounding boxes.\n",
    "        originalDim : torch.Tensor\n",
    "             a tensor of original dimensions (width, height) of the bounding boxes.\n",
    "        normalizeBboxes :torch.Tensor\n",
    "             a tensor of normalized bounding box images.\n",
    "        \"\"\"\n",
    "        self.bboxesImages = None\n",
    "        self.topLeft = None\n",
    "        self.originalDim = None\n",
    "        self.normalizeBboxes = None\n",
    "\n",
    "    def _extractBoundingBoxes(self, img: torch.Tensor, bboxes: List[List[int]]):\n",
    "        \"\"\"Crop the image according to the bounding boxes.\n",
    "        Parameters:\n",
    "        -----------\n",
    "            img: np.ndarray\n",
    "                Image to be cropped.\n",
    "            bboxes: np.ndarray\n",
    "                Bounding boxes to crop the image.\n",
    "        \"\"\"\n",
    "        bbox = []\n",
    "        bboxesImages = []\n",
    "        topLeft = []\n",
    "        originalDim = []\n",
    "\n",
    "        for box in bboxes:\n",
    "            x1, y1 = box[0], box[1]\n",
    "            x2, y2 = box[2], box[3]\n",
    "            bbox = img[y1:y2, x1:x2]\n",
    "\n",
    "            topLeft.append((x1, y1))\n",
    "            originalDim.append((bbox.shape[1], bbox.shape[0]))\n",
    "            bboxesImages.append(bbox)\n",
    "\n",
    "        boundingBoxTransformationMatrix = torch.from_numpy(np.array(topLeft))\n",
    "        originalBboxesDimsMatrix = torch.from_numpy(np.array(originalDim))\n",
    "\n",
    "        return bboxesImages, boundingBoxTransformationMatrix, originalBboxesDimsMatrix\n",
    "\n",
    "    def _resizeNormalizeBboxes(self, bboxesImages, width: int, height: int):\n",
    "        \"\"\"Resize and normalize the bounding boxes.\n",
    "        Parameters:\n",
    "        -----------\n",
    "            width: int\n",
    "                Width of the resized image.\n",
    "            height: int\n",
    "                Height of the resized image.\n",
    "        \"\"\"\n",
    "        transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((width, height)),\n",
    "                transforms.Grayscale()\n",
    "                #transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "            ]\n",
    "        )\n",
    "        resizedBboxes = bboxesImages[0].float().permute(2, 0, 1)\n",
    "        resizedBboxes = transform(resizedBboxes / 255).unsqueeze(0)\n",
    "\n",
    "        for img in bboxesImages[1:]:\n",
    "            img = img.permute(2, 0, 1).float()\n",
    "            img = transform(img / 255)\n",
    "            resizedBboxes = torch.cat((resizedBboxes, img.unsqueeze(0)), dim=0)\n",
    "\n",
    "        return resizedBboxes\n",
    "\n",
    "    def preprocessBboxes(self, img, width: int, height: int, bboxes: List[List[int]]):\n",
    "        \"\"\"Preprocess the bounding boxes.\n",
    "        Parameters:\n",
    "        -----------\n",
    "            imgPath: str\n",
    "                Path to the image file.\n",
    "            width: int\n",
    "                Width of the resized image.\n",
    "            height: int\n",
    "                Height of the resized image.\n",
    "            bboxes: np.ndarray\n",
    "                Bounding boxes to crop the image.\n",
    "        Returns:\n",
    "        --------\n",
    "            resized_bboxes: torch.Tensor\n",
    "                Tensor of normalized bounding box images.\n",
    "        \"\"\"\n",
    "        # img=self.loadImage(imgPath)\n",
    "        img = torch.from_numpy(img)\n",
    "        bboxesImages, boundingBoxTransformationMatrix, originalBboxesDimsMatrix = self._extractBoundingBoxes(\n",
    "            img, bboxes\n",
    "        )\n",
    "        bboxesImages = self._resizeNormalizeBboxes(bboxesImages, width, height)\n",
    "        return bboxesImages, boundingBoxTransformationMatrix, originalBboxesDimsMatrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "\n",
    "def plot_bounding_boxes(img, bounding_boxes):\n",
    "    # Open the image file\n",
    "\n",
    "    # Create a figure and axis object\n",
    "    fig, ax = plt.subplots(1)\n",
    "\n",
    "    # Display the image\n",
    "    ax.imshow(img)\n",
    "\n",
    "    # Add the bounding boxes to the plot\n",
    "    for bbox in bounding_boxes:\n",
    "        # Extract the coordinates from the bounding box\n",
    "        x, y, w, h = bbox\n",
    "\n",
    "        # Create a rectangle patch\n",
    "        rect = patches.Rectangle((x, y), w, h, linewidth=1, edgecolor='r', facecolor='none')\n",
    "\n",
    "        # Add the rectangle patch to the axis\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "image_proccess=ImagePreprocessing()\n",
    "bboxes =  np.array([[88,147,80+30,150+147]])\n",
    "img = plt.imread('121.jpeg')\n",
    "plot_bounding_boxes(img, bboxes)\n",
    "print(img.shape)\n",
    "plt.imshow(img)\n",
    "# return\n",
    "res=image_proccess.preprocessBboxes(img,80,80,bboxes)\n",
    "# print(image_proccess.originalDim)\n",
    "# print(len(ready_images))\n",
    "# print(image_proccess.topLeft)\n",
    "# print(ready_images.shape)\n",
    "# # print(ready_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res[0].shape, res[1].shape, res[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kps = model(res[0].cuda())\n",
    "print(kps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import numpy.typing as np\n",
    "\n",
    "\n",
    "def postProcessing(\n",
    "    keyPointArray,\n",
    "    imageSize,\n",
    "    topLeftBbox,\n",
    "):\n",
    "    \"\"\"\n",
    "    This function is used to postprocess the output of the neural network.\n",
    "\n",
    "        Parameters:\n",
    "        keyPointArray (numpy.ndarray): The output of the neural network.\n",
    "        imageSize (numpy.ndarray): The size of the image.\n",
    "        topLeftBbox (numpy.ndarray): The top left corner of the image.\n",
    "\n",
    "        Returns:\n",
    "        numpy.ndarray: The postprocessed output of the neural network.\n",
    "    \"\"\"\n",
    "    keyPointArray = keyPointArray.view(-1,2)\n",
    "    return (keyPointArray*imageSize+topLeftBbox).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postProcessing(kps.cpu().detach(), res[1], res[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
